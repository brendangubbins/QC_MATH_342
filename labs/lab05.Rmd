---
title: "Lab 5"
author: "Brendan Gubbins"
output: pdf_document
date: "11:59PM March 18, 2021"
---


Create a 2x2 matrix with the first column 1's and the next column iid normals. Find the absolute value of the angle (in degrees, not radians) between the two columns.

```{r}
norm_vec = function(v) {
  sqrt(sum(v^2))
}

X = matrix(1, nrow = 2, ncol = 2)
X[,2] = rnorm(2)
X

cos_theta = (t(X[,1]) %*% X[,2]) / (norm_vec(X[,1]) * norm_vec(X[,2]))
cos_theta
abs(90 - acos(cos_theta) * 180/pi)
```

Repeat this exercise `Nsim = 1e5` times and report the average absolute angle.

```{r}
Nsim = 1e5
angles = array(NA, Nsim)

for (i in 1:Nsim) {
  X = matrix(1, nrow = 2, ncol = 2)
  X[,2] = rnorm(2)
  cos_theta = t(X[,1]) %*% X[,2] / (norm_vec(X[,1]) %*% norm_vec(X[,2]))
  angles[i] = abs(90 - acos(cos_theta) * 180/pi)
}

mean(angles)
```

Create a 2xn matrix with the first column 1's and the next column iid normals. Find the absolute value of the angle (in degrees, not radians) between the two columns. For n = 10, 50, 100, 200, 500, 1000, report the average absolute angle over `Nsim = 1e5` simulations.

```{r}
#Ns = c(2, 5, 10, 50, 100, 200, 500, 1000)
#starts at 45 degrees

Ns = c(10, 50, 100, 200, 500, 1000)
Nsim = 1e5
angles = matrix(NA, nrow = Nsim, ncol = length(Ns))

for (j in 1:length(Ns)) {
  for (i in 1:Nsim) {
    X = matrix(1, nrow = Ns[j], ncol = 2)
    X[,2] = rnorm(Ns[j])
    cos_theta = t(X[,1]) %*% X[,2] / (norm_vec(X[,1]) %*% norm_vec(X[,2]))
    angles[i,j] = abs(90 - acos(cos_theta) * 180/pi)
  }
}

colMeans(angles)
```

What is this absolute angle converging to? Why does this make sense?

The absolute angle difference from 90 is converging to 0. It makes sense because in a high dimensional space, random directions are orthogonal

Create a vector y by simulating n = 100 standard iid normals. Create a matrix of size 100 x 2 and populate the first column by all ones (for the intercept) and the second column by 100 standard iid normals. Find the R^2 of an OLS regression of `y ~ X`. Use matrix algebra.

```{r}
n = 100
X = cbind(1, rnorm(n))
y = rnorm(n)

H = X %*% solve((t(X) %*% X)) %*% t(X)
y_hat = H %*% y
y_bar = mean(y)

SSR = sum((y_hat - y_bar)^2)
SST = sum((y - y_bar)^2)

Rsq = SSR / SST
Rsq
```

Write a for loop to each time bind a new column of 100 standard iid normals to the matrix X and find the R^2 each time until the number of columns is 100. Create a vector to save all R^2's. What happened??

```{r}
Rsqs = array(NA, dim = n - 2)

for (j in 1:(n - 2)) {
  X = cbind(X, rnorm(n))
  H = X %*% solve((t(X) %*% X)) %*% t(X)
  y_hat = H %*% y
  y_bar = mean(y)
  
  SSR = sum((y_hat - y_bar)^2)
  SST = sum((y - y_bar)^2)
  
  Rsqs[j] = SSR / SST
}

Rsqs
diff(Rsqs)
```

Test that the projection matrix onto this X is the same as I_n. You may have to vectorize the matrices in the `expect_equal` function for the test to work.

```{r}
pacman::p_load(testthat)

dim(X)
H = X %*% solve((t(X) %*% X)) %*% t(X)
I = diag(n)

expect_equal(H, I)
```

Add one final column to X to bring the number of columns to 101. Then try to compute R^2. What happens? 

```{r eval = FALSE}
X = cbind(X, rnorm(n))
H = X %*% solve((t(X) %*% X)) %*% t(X) # this is the error
y_hat = H %*% y
y_bar = mean(y)
  
SSR = sum((y_hat - y_bar)^2)
SST = sum((y - y_bar)^2)
  
Rsq = SSR / SST
Rsq
```

Why does this make sense?

The computation for H results in an error. This is because X transpose X is a rank deficient matrix, therefore it is not invertible.

Write a function spec'd as follows:

```{r}
#' Orthogonal Projection
#'
#' Projects vector a onto v.
#'
#' @param a   the vector to project
#' @param v   the vector projected onto
#'
#' @returns   a list of two vectors, the orthogonal projection parallel to v named a_parallel, 
#'            and the orthogonal error orthogonal to v called a_perpendicular
orthogonal_projection = function(a, v){
  H = (v %*% t(v)) / norm_vec(v)^2
  a_parallel = H %*% a
  a_perpendicular = a - a_parallel
  
  list(a_parallel = a_parallel, a_perpendicular = a_perpendicular)
}
```

Provide predictions for each of these computations and then run them to make sure you're correct.

```{r}
orthogonal_projection(c(1,2,3,4), c(1,2,3,4))
#prediction: parallel same, perpendicular zero
orthogonal_projection(c(1, 2, 3, 4), c(0, 2, 0, -1))
#prediction: parallel zero
result = orthogonal_projection(c(2, 6, 7, 3), c(1, 3, 5, 7))
t(result$a_parallel) %*% result$a_perpendicular
#prediction: zero
result$a_parallel + result$a_perpendicular
#prediction: original vector
result$a_parallel / c(1, 3, 5 ,7)
#prediction: smaller percentage of projection
```

Let's use the Boston Housing Data for the following exercises

```{r}
y = MASS::Boston$medv
X = model.matrix(medv ~ ., MASS::Boston)
p_plus_one = ncol(X)
n = nrow(X)
head(X)
```

Using your function `orthogonal_projection` orthogonally project onto the column space of X by projecting y on each vector of X individually and adding up the projections and call the sum `yhat_naive`.

```{r}
yhat_naive = rep(0, n)

for (j in 1:p_plus_one) {
  yhat_naive = yhat_naive + orthogonal_projection(y, X[,j])$a_parallel
}
```

How much double counting occurred? Measure the magnitude relative to the true LS orthogonal projection.

```{r}
y_hat = X %*% solve((t(X) %*% X)) %*% t(X) %*% y
sqrt(sum(yhat_naive^2)) / sqrt(sum(y_hat^2))
```

Is this ratio expected? Why or why not?

It's expected to be different from 1.

Convert X into V where V has the same column space as X but has orthogonal columns. You can use the function `orthogonal_projection`. This is the Gram-Schmidt orthogonalization algorithm.

```{r}
V = matrix(NA, nrow = n, ncol = p_plus_one)
V[ , 1] = X[ , 1]
for (j in 2:p_plus_one) {
  V[,j] = X[,j]# - orthogonal_projection(X[,j], V[,j-1])$a_parallel
  for (k in 1:(j-1)) {
    V[,j] = V[,j] - orthogonal_projection(X[,j], V[,k])$a_parallel
  }
}

V[,7] %*% V[,9]
```

Convert V into Q whose columns are the same except normalized

```{r}
Q = matrix(NA, nrow = n, ncol = p_plus_one)
for (j in 1:p_plus_one) {
  Q[,j] = V[,j] / norm_vec(V[,j])
}
```

Verify Q^T Q is I_{p+1} i.e. Q is an orthonormal matrix.

```{r}
expect_equal(t(Q) %*% Q, diag(p_plus_one))
```

Is your Q the same as what results from R's built-in QR-decomposition function?

```{r}
Q_from_Rs_builtin = qr.Q(qr(X))
#expect_equal(Q_from_Rs_builtin, Q) THEY ARE NOT EQUAL!
```
 
Is this expected? Why did this happen?

This is expected. There are many orthonormal basis of any column space. The projection will still be the same.

Project y onto colsp[Q] and verify it is the same as the OLS fit. You may have to use the function `unname` to compare the vectors since they the entries will likely have different names.

```{r}
proj = unname(lm(y_hat ~ Q)$fitted.values)
expect_equal(proj, c(unname(y_hat)))
```

Project y onto colsp[Q] one by one and verify it sums to be the projection onto the whole space.

```{r}
yhat_naive = 0

for (j in 1:p_plus_one) {
  yhat_naive = yhat_naive + orthogonal_projection(y_hat, Q[,j])$a_parallel
}

expect_equal(unname(yhat_naive), unname(y_hat))
```

Split the Boston Housing Data into a training set and a test set where the training set is 80% of the observations. Do so at random.

```{r}
K = 5
n_test = round(n * 1 / K)
n_train = n - n_test

test_indices = sample(1 : n, n_test)
train_indices = setdiff(1 : n, test_indices)

X_train = X[train_indices,]
y_train = y[train_indices]

X_test = X[test_indices,]
y_test = y[test_indices]
```

Fit an OLS model. Find the s_e in sample and out of sample. Which one is greater? Note: we are now using s_e and not RMSE since RMSE has the n-(p + 1) in the denominator not n-1 which attempts to de-bias the error estimate by inflating the estimate when overfitting in high p. Again, we're just using `sd(e)`, the sample standard deviation of the residuals.

```{r}
ols_mod = lm(y_train ~ .+0, data.frame(X_train))

s_e = sd(ols_mod$residuals)
s_e

y_oos = predict(ols_mod, data.frame(X_test))
residuals = y_test - y_oos
ooss_e = sd(residuals)
ooss_e

# s_e of oos residuals is greater (when I ran it)
# it is sometimes different depending on the sample from above chunk
```

Do these two exercises `Nsim = 1000` times and find the average difference between s_e and ooss_e. 

```{r}
Nsim = 1000
sum = 0

for (i in 1:Nsim) {
  test_indices = sample(1 : n, n_test)
  train_indices = setdiff(1 : n, test_indices)

  X_train = X[train_indices,]
  y_train = y[train_indices]

  X_test = X[test_indices,]
  y_test = y[test_indices]
  
  ols_mod = lm(y_train ~ .+0, data.frame(X_train))
  
  s_e = sd(ols_mod$residuals)
  
  y_oos = predict(ols_mod, data.frame(X_test))
  residuals = y_test - y_oos
  
  ooss_e = sd(residuals)
  
  sum = sum + abs(s_e - ooss_e)
}

avg_diff = sum / Nsim
avg_diff # not too much difference, makes sense
```

We'll now add random junk to the data so that `p_plus_one = n_train` and create a new data matrix `X_with_junk.`

```{r}
X_with_junk = cbind(X, matrix(rnorm(n * (n_train - p_plus_one)), nrow = n))
dim(X)
dim(X_with_junk)
```

Repeat the exercise above measuring the average s_e and ooss_e but this time record these metrics by number of features used. That is, do it for the first column of `X_with_junk` (the intercept column), then do it for the first and second columns, then the first three columns, etc until you do it for all columns of `X_with_junk`. Save these in `s_e_by_p` and `ooss_e_by_p`.


```{r}
test_indices = sample(1 : n, n_test)
train_indices = setdiff(1 : n, test_indices)

s_e_by_p = rep(NA, ncol(X_with_junk))
ooss_e_by_p = rep(NA, ncol(X_with_junk))

sum_by_p = 0
oos_sum_by_p = 0
Nsim = 100 # runtime is too long for 1000

for (i in 1 : Nsim) {
  
  for (j in 1 : ncol(X_with_junk)) {
  
  X_train = X_with_junk[train_indices, 1 : j, drop = FALSE]
  y_train = y[train_indices]
  
  X_test = X_with_junk[test_indices, 1 : j, drop = FALSE]
  y_test = y[test_indices]
    
  in_mod = lm(y_train ~ .+0, data.frame(X_train))
  oos_y = predict(in_mod, data.frame(X_test))

  s_e_by_p[j] = sd(in_mod$residuals)
  ooss_e_by_p[j] = sd(y_test - oos_y)
  }
  
  sum_by_p = sum_by_p + sum(s_e_by_p)
  oos_sum_by_p = oos_sum_by_p + sum(ooss_e_by_p)
}

sum_by_p / (ncol(X_with_junk) * Nsim) # average diff
oos_sum_by_p / (ncol(X_with_junk) * Nsim) # average diff oos
```

You can graph them here:

```{r}
pacman::p_load(ggplot2)
ggplot(
  rbind(
    data.frame(s_e = s_e_by_p, p = 1 : n_train, series = "in-sample"),
    data.frame(s_e = ooss_e_by_p, p = 1 : n_train, series = "out-of-sample")
  )) +
  geom_line(aes(x = p, y = s_e, col = series))
```
 
Is this shape expected? Explain.

This shape is expected, as you can see the model predicts in-sample and out-of-sample similarly at low p (columns). As more junk columns are added to the matrix, the in-sample error continuous to go down until p = n, where the error is 0. This is overfitting. For the out-of-sample error, as p increases, the error increases. The model fails to predict out-of-sample when there is a large amount of junk columns added. This is another consequence of overfitting.